custom:

  # Cluster configs for each environment
  default-cluster-spec: &default-cluster-spec
    spark_version: '12.2.x-cpu-ml-scala2.12'
    node_type_id: 'Standard_DS3_v2' # NOTE: this is an Azure-specific instance type. Change accordingly if running on Azure or GCP.
    driver_node_type_id: 'Standard_DS3_v2'  # NOTE: this is an Azure-specific instance type. Change accordingly if running on Azure or GCP.
    data_security_mode: 'SINGLE_USER'
    num_workers: 1
    # To reduce start up time for each job, it is advisable to use a cluster pool. To do so involves supplying the following
    # two fields with a pool_id to acquire both the driver and instances from.
    # If driver_instance_pool_id and instance_pool_id are set, both node_type_id and driver_node_type_id CANNOT be supplied.
    # As such, if providing a pool_id for driver and worker instances, please ensure that node_type_id and driver_node_type_id are not present
#    driver_instance_pool_id: '0617-151415-bells2-pool-hh7h6tjm'
#    instance_pool_id: '0617-151415-bells2-pool-hh7h6tjm'

  dev-cluster-config: &dev-cluster-config
    new_cluster:
      <<: *default-cluster-spec

  staging-cluster-config: &staging-cluster-config
    new_cluster:
      <<: *default-cluster-spec

  prod-cluster-config: &prod-cluster-config
    new_cluster:
      <<: *default-cluster-spec

# Databricks Jobs definitions
# please note that we're using FUSE reference for config, and env files, hence we're going to load this file using its local FS path
environments:

  dev:
    strict_path_adjustment_policy: true
    workflows:
      - name: 'DEV-telco-churn-demo-setup-pj'
        <<: *dev-cluster-config
        notebook_task:
          notebook_path: '/Repos/anastasia.prokaieva@databricks.com/daiwt_mlops_Paris/notebooks/demo_setup'
          parameters: ['--base-data-params', 'file:fuse://conf/.base_data_params.env',
                       '--env', 'file:fuse://conf/dev/.dev.env',
                       '--conf-file', 'file:fuse://conf/pipeline_configs/demo_setup.yml']
      - name: 'DEV-telco-churn-feature-table-creation-pj'
        <<: *dev-cluster-config
        notebook_task:
          notebook_path: '/Repos/anastasia.prokaieva@databricks.com/daiwt_mlops_Paris/notebooks/feature_table_creator'
          parameters: ['--base-data-params', 'file:fuse://conf/.base_data_params.env',
                       '--env', 'file:fuse://conf/dev/.dev.env',
                       '--conf-file', 'file:fuse://conf/pipeline_configs/feature_table_creator.yml']
      - name: 'DEV-telco-churn-model-train-pj'
        <<:
          - *dev-cluster-config
        notebook_task:
          notebook_path: '/Repos/anastasia.prokaieva@databricks.com/daiwt_mlops_Paris/notebooks/model_train'
          parameters: ['--base-data-params', 'file:fuse://conf/.base_data_params.env',
                       '--env', 'file:fuse://conf/dev/.dev.env',
                       '--conf-file', 'file:fuse://conf/pipeline_configs/model_train.yml']
      - name: 'DEV-telco-churn-model-deployment-pj'
        <<:
          - *dev-cluster-config
        notebook_task:
          notebook_path: '/Repos/anastasia.prokaieva@databricks.com/daiwt_mlops_Paris/notebooks/model_deployment'
          parameters: ['--base-data-params', 'file:fuse://conf/.base_data_params.env',
                       '--env', 'file:fuse://conf/dev/.dev.env',
                       '--conf-file', 'file:fuse://conf/pipeline_configs/model_deployment.yml']
      - name: 'DEV-telco-churn-model-inference-batch-pj'
        <<:
          - *dev-cluster-config
        notebook_task:
          notebook_path: '/Repos/anastasia.prokaieva@databricks.com/daiwt_mlops_Paris/notebooks/model_inference_batch'
          parameters: ['--base-data-params', 'file:fuse://conf/.base_data_params.env',
                       '--env', 'file:fuse://conf/dev/.dev.env',
                       '--conf-file', 'file:fuse://conf/pipeline_configs/model_inference_batch.yml']
      - name: 'DEV-telco-churn-sample-integration-test-pj'
        <<:
          - *dev-cluster-config
        spark_python_task:
          python_file: 'file://tests/integration/sample_test.py'
          parameters: ['--base-data-params', 'file:fuse://conf/.base_data_params.env',
                       '--env', 'file:fuse://conf/dev/.dev.env',
                       '--conf-file', 'file:fuse://conf/pipeline_configs/sample_test.yml']

  staging:
    workflows:
      - name: 'STAGING-telco-churn-sample-integration-test-pj'
        <<:
          - *staging-cluster-config
        max_concurrent_runs: 5
        spark_python_task:
          python_file: 'file://tests/integration/sample_test.py'
          parameters: ['--env', 'file:fuse://conf/staging/.staging.env',
                       '--conf-file', 'file:fuse://conf/pipeline_configs/sample_test.yml']

  prod:
    strict_path_adjustment_policy: true
    workflows:
      - name: 'PROD-telco-churn-demo-setup-pj'
        <<: *prod-cluster-config
        git_source:
          git_url: https://github.com/puneet-jain159/daiwt_mlops_Paris.git
          git_provider: "github"
          git_branch: "main"
        notebook_task:
          notebook_path: 'notebooks/demo_setup'
          parameters: ['--base-data-params', 'file:fuse://conf/.base_data_params.env',
                       '--env', 'file:fuse://conf/prod/.prod.env',
                       '--conf-file', 'file:fuse://conf/pipeline_configs/demo_setup.yml']

      - name: 'PROD-telco-churn-initial-model-train-register-pj'
        job_clusters:
          - job_cluster_key: "prod_mlops_cluster_pj"
            <<: *prod-cluster-config
        tasks:
          - task_key: 'demo-setup'
            job_cluster_key: "prod_mlops_cluster_pj"
            spark_python_task:
              python_file: 'file://telco_churn/pipelines/demo_setup_job.py'
              parameters: ['--base-data-params', 'file:fuse://conf/.base_data_params.env',
                           '--env', 'file:fuse://conf/prod/.prod.env',
                           '--conf-file', 'file:fuse://conf/pipeline_configs/demo_setup.yml']
          - task_key: 'feature-table-creation'
            job_cluster_key: "prod_mlops_cluster_pj"
            depends_on:
              - task_key: 'demo-setup'
            spark_python_task:
              python_file: 'file://telco_churn/pipelines/feature_table_creator_job.py'
              parameters: ['--base-data-params', 'file:fuse://conf/.base_data_params.env',
                           '--env', 'file:fuse://conf/prod/.prod.env',
                           '--conf-file', 'file:fuse://conf/pipeline_configs/feature_table_creator.yml']
          - task_key: 'model-train'
            job_cluster_key: "prod_mlops_cluster_pj"
            depends_on:
              - task_key: 'demo-setup'
              - task_key: 'feature-table-creation'
            spark_python_task:
              python_file: 'file://telco_churn/pipelines/model_train_job.py'
              parameters: ['--base-data-params', 'file:fuse://conf/.base_data_params.env',
                           '--env', 'file:fuse://conf/prod/.prod.env',
                           '--conf-file', 'file:fuse://conf/pipeline_configs/model_train.yml']

      - name: 'PROD-telco-churn-train-deploy-inference-workflow-pj'
        job_clusters:
          - job_cluster_key: "prod_mlops_cluster_pj"
            <<: *prod-cluster-config
        git_source:
          git_url: https://github.com/puneet-jain159/daiwt_mlops_Paris.git
          git_provider: "github"
          git_branch: "main"
        tasks:
          - task_key: 'model-train'
            job_cluster_key: "prod_mlops_cluster_pj"
            notebook_task:
              notebook_path: 'notebooks/model_train'
              base_parameters: {"env" : "prod"}

          - task_key: 'model-deploy'
            job_cluster_key: "prod_mlops_cluster_pj"
            notebook_task:
              notebook_path: 'notebooks/model_deployment'
              base_parameters: {"env" : "prod"}
            depends_on:
              - task_key: 'model-train'

          - task_key: 'model-inference-batch'
            job_cluster_key: "prod_mlops_cluster_pj"
            notebook_task:
              notebook_path: 'notebooks/model_inference_batch'
              base_parameters: {"env" : "prod"}
            depends_on:
              - task_key: 'model-train'
              - task_key: 'model-deploy'

      - name: 'PROD-telco-churn-model-train-pj'
        job_clusters:
          - job_cluster_key: "prod_mlops_cluster_pj"
            <<: *prod-cluster-config
        git_source:
          git_url: https://github.com/puneet-jain159/daiwt_mlops_Paris.git
          git_provider: "github"
          git_branch: "main"

        tasks:
          - task_key: 'model-train'
            job_cluster_key: "prod_mlops_cluster_pj"
            notebook_task:
              base_parameters: {"env" : "prod"}
              notebook_path: 'notebooks/model_train'
              base_parameters: {"env" : "prod"}
              parameters: ['--base-data-params', 'file:fuse://conf/.base_data_params.env',
                          '--env', 'file:fuse://conf/prod/.prod.env',
                          '--conf-file', 'file:fuse://conf/pipeline_configs/model_train.yml']
              source: GIT

      - name: 'PROD-telco-churn-model-deployment-pj'
        job_clusters:
          - job_cluster_key: "prod_mlops_cluster_pj"
            <<: *prod-cluster-config
        git_source:
          git_url: https://github.com/puneet-jain159/daiwt_mlops_Paris.git
          git_provider: "github"
          git_branch: "main"

        tasks:
          - task_key: 'model_deployment'
            job_cluster_key: "prod_mlops_cluster_pj"
            notebook_task:
              base_parameters: {"env" : "prod"}
              notebook_path: 'notebooks/model_deployment'
              parameters: ['--base-data-params', 'file:fuse://conf/.base_data_params.env',
                          '--env', 'file:fuse://conf/prod/.prod.env',
                          '--conf-file', 'file:fuse://conf/pipeline_configs/model_deployment.yml']
              source: GIT

      - name: 'PROD-telco-churn-model-inference-batch-pj'
        job_clusters:
          - job_cluster_key: "prod_mlops_cluster_pj"
            <<: *prod-cluster-config
        git_source:
          git_url: https://github.com/puneet-jain159/daiwt_mlops_Paris.git
          git_provider: "github"
          git_branch: "main"

        tasks:
          - task_key: 'model_inference_batch'
            job_cluster_key: "prod_mlops_cluster_pj"
            notebook_task:
              notebook_path: 'notebooks/model_inference_batch'
              base_parameters: {"env" : "prod"}
              parameters: ['--base-data-params', 'file:fuse://conf/.base_data_params.env',
                          '--env', 'file:fuse://conf/prod/.prod.env',
                          '--conf-file', 'file:fuse://conf/pipeline_configs/model_inference_batch.yml']
              source: GIT
